{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c792a610-ff83-4b52-b0c9-c24c2a4c6b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 15:58:40,736 P20988 INFO Using GPU...\n",
      "2025-04-25 15:58:40,737 P20988 INFO Load from ./chunks\\SN\n",
      "2025-04-25 15:58:46,934 P20988 INFO Model Parameters: hash_id=b52b1a7d, data=SN, device=cuda, lr=0.001, epoches=70\n",
      "2025-04-25 15:58:54,830 P20988 INFO Epoch 1/70, training loss: 1.39945 [7.89s]\n",
      "2025-04-25 15:58:58,060 P20988 INFO Epoch 2/70, training loss: 0.81892 [3.23s]\n",
      "2025-04-25 15:59:01,316 P20988 INFO Epoch 3/70, training loss: 0.24630 [3.25s]\n",
      "2025-04-25 15:59:04,627 P20988 INFO Epoch 4/70, training loss: 0.15544 [3.31s]\n",
      "2025-04-25 15:59:07,931 P20988 INFO Epoch 5/70, training loss: 0.09936 [3.30s]\n",
      "2025-04-25 15:59:11,257 P20988 INFO Epoch 6/70, training loss: 0.06911 [3.32s]\n",
      "2025-04-25 15:59:14,555 P20988 INFO Epoch 7/70, training loss: 0.04737 [3.30s]\n",
      "2025-04-25 15:59:17,827 P20988 INFO Epoch 8/70, training loss: 0.04054 [3.27s]\n",
      "2025-04-25 15:59:21,120 P20988 INFO Epoch 9/70, training loss: 0.03548 [3.29s]\n",
      "2025-04-25 15:59:27,244 P20988 INFO  testing loss: 0.03953 \n",
      "2025-04-25 15:59:27,245 P20988 INFO Test -- F1: 0.9900, Rec: 0.9915, Pre: 0.9886, HR@1: 0.9836, ndcg@1: 0.9836, HR@3: 0.9915, ndcg@3: 0.9885, HR@5: 0.9915, ndcg@5: 0.9885\n",
      "2025-04-25 15:59:30,784 P20988 INFO Epoch 10/70, training loss: 0.02656 [3.50s]\n",
      "2025-04-25 15:59:34,017 P20988 INFO Epoch 11/70, training loss: 0.02721 [3.23s]\n",
      "2025-04-25 15:59:37,320 P20988 INFO Epoch 12/70, training loss: 0.02306 [3.30s]\n",
      "2025-04-25 15:59:40,492 P20988 INFO Epoch 13/70, training loss: 0.02838 [3.17s]\n",
      "2025-04-25 15:59:43,757 P20988 INFO Epoch 14/70, training loss: 0.09873 [3.26s]\n",
      "2025-04-25 15:59:46,981 P20988 INFO Epoch 15/70, training loss: 0.08163 [3.22s]\n",
      "2025-04-25 15:59:50,225 P20988 INFO Epoch 16/70, training loss: 0.04088 [3.24s]\n",
      "2025-04-25 15:59:53,558 P20988 INFO Epoch 17/70, training loss: 0.03058 [3.33s]\n",
      "2025-04-25 15:59:56,757 P20988 INFO Epoch 18/70, training loss: 0.02330 [3.20s]\n",
      "2025-04-25 15:59:59,995 P20988 INFO Epoch 19/70, training loss: 0.01878 [3.24s]\n",
      "2025-04-25 16:00:05,769 P20988 INFO  testing loss: 0.03825 \n",
      "2025-04-25 16:00:05,770 P20988 INFO Test -- F1: 0.9882, Rec: 0.9843, Pre: 0.9921, HR@1: 0.9836, ndcg@1: 0.9836, HR@3: 0.9843, ndcg@3: 0.9841, HR@5: 0.9843, ndcg@5: 0.9841\n",
      "2025-04-25 16:00:09,328 P20988 INFO Epoch 20/70, training loss: 0.01582 [3.54s]\n",
      "2025-04-25 16:00:12,428 P20988 INFO Epoch 21/70, training loss: 0.01720 [3.10s]\n",
      "2025-04-25 16:00:15,695 P20988 INFO Epoch 22/70, training loss: 0.01617 [3.26s]\n",
      "2025-04-25 16:00:18,988 P20988 INFO Epoch 23/70, training loss: 0.01863 [3.29s]\n",
      "2025-04-25 16:00:22,288 P20988 INFO Epoch 24/70, training loss: 0.02146 [3.30s]\n",
      "2025-04-25 16:00:25,470 P20988 INFO Epoch 25/70, training loss: 0.02748 [3.18s]\n",
      "2025-04-25 16:00:28,741 P20988 INFO Epoch 26/70, training loss: 0.03132 [3.27s]\n",
      "2025-04-25 16:00:31,991 P20988 INFO Epoch 27/70, training loss: 0.03896 [3.25s]\n",
      "2025-04-25 16:00:35,193 P20988 INFO Epoch 28/70, training loss: 0.02633 [3.20s]\n",
      "2025-04-25 16:00:38,471 P20988 INFO Epoch 29/70, training loss: 0.01495 [3.28s]\n",
      "2025-04-25 16:00:44,309 P20988 INFO  testing loss: 0.03136 \n",
      "2025-04-25 16:00:44,310 P20988 INFO Test -- F1: 0.9897, Rec: 0.9893, Pre: 0.9900, HR@1: 0.9893, ndcg@1: 0.9893, HR@3: 0.9893, ndcg@3: 0.9893, HR@5: 0.9893, ndcg@5: 0.9893\n",
      "2025-04-25 16:00:47,667 P20988 INFO Epoch 30/70, training loss: 0.00903 [3.32s]\n",
      "2025-04-25 16:00:50,909 P20988 INFO Epoch 31/70, training loss: 0.00596 [3.24s]\n",
      "2025-04-25 16:00:54,211 P20988 INFO Epoch 32/70, training loss: 0.00509 [3.30s]\n",
      "2025-04-25 16:00:57,443 P20988 INFO Epoch 33/70, training loss: 0.00536 [3.23s]\n",
      "2025-04-25 16:01:00,728 P20988 INFO Epoch 34/70, training loss: 0.00329 [3.28s]\n",
      "2025-04-25 16:01:03,968 P20988 INFO Epoch 35/70, training loss: 0.00297 [3.24s]\n",
      "2025-04-25 16:01:07,266 P20988 INFO Epoch 36/70, training loss: 0.00512 [3.30s]\n",
      "2025-04-25 16:01:10,547 P20988 INFO Epoch 37/70, training loss: 0.00444 [3.28s]\n",
      "2025-04-25 16:01:13,784 P20988 INFO Epoch 38/70, training loss: 0.00394 [3.24s]\n",
      "2025-04-25 16:01:17,064 P20988 INFO Epoch 39/70, training loss: 0.00788 [3.28s]\n",
      "2025-04-25 16:01:22,821 P20988 INFO  testing loss: 0.03454 \n",
      "2025-04-25 16:01:22,821 P20988 INFO Test -- F1: 0.9929, Rec: 0.9922, Pre: 0.9936, HR@1: 0.9915, ndcg@1: 0.9915, HR@3: 0.9922, ndcg@3: 0.9919, HR@5: 0.9922, ndcg@5: 0.9919\n",
      "2025-04-25 16:01:26,305 P20988 INFO Epoch 40/70, training loss: 0.00322 [3.44s]\n",
      "2025-04-25 16:01:29,538 P20988 INFO Epoch 41/70, training loss: 0.00371 [3.23s]\n",
      "2025-04-25 16:01:32,761 P20988 INFO Epoch 42/70, training loss: 0.00831 [3.22s]\n",
      "2025-04-25 16:01:36,046 P20988 INFO Epoch 43/70, training loss: 0.01066 [3.28s]\n",
      "2025-04-25 16:01:39,339 P20988 INFO Epoch 44/70, training loss: 0.00926 [3.29s]\n",
      "2025-04-25 16:01:42,603 P20988 INFO Epoch 45/70, training loss: 0.00703 [3.26s]\n",
      "2025-04-25 16:01:45,888 P20988 INFO Epoch 46/70, training loss: 0.00957 [3.28s]\n",
      "2025-04-25 16:01:49,184 P20988 INFO Epoch 47/70, training loss: 0.00676 [3.29s]\n",
      "2025-04-25 16:01:52,462 P20988 INFO Epoch 48/70, training loss: 0.01211 [3.28s]\n",
      "2025-04-25 16:01:55,736 P20988 INFO Epoch 49/70, training loss: 0.00991 [3.27s]\n",
      "2025-04-25 16:02:01,526 P20988 INFO  testing loss: 0.02408 \n",
      "2025-04-25 16:02:01,528 P20988 INFO Test -- F1: 0.9940, Rec: 0.9964, Pre: 0.9915, HR@1: 0.9957, ndcg@1: 0.9957, HR@3: 0.9957, ndcg@3: 0.9957, HR@5: 0.9964, ndcg@5: 0.9960\n",
      "2025-04-25 16:02:05,089 P20988 INFO Epoch 50/70, training loss: 0.00634 [3.52s]\n",
      "2025-04-25 16:02:08,303 P20988 INFO Epoch 51/70, training loss: 0.00819 [3.21s]\n",
      "2025-04-25 16:02:11,366 P20988 INFO Epoch 52/70, training loss: 0.01128 [3.06s]\n",
      "2025-04-25 16:02:14,565 P20988 INFO Epoch 53/70, training loss: 0.01358 [3.20s]\n",
      "2025-04-25 16:02:17,827 P20988 INFO Epoch 54/70, training loss: 0.03418 [3.26s]\n",
      "2025-04-25 16:02:21,122 P20988 INFO Epoch 55/70, training loss: 0.03328 [3.29s]\n",
      "2025-04-25 16:02:24,355 P20988 INFO Epoch 56/70, training loss: 0.03266 [3.23s]\n",
      "2025-04-25 16:02:27,602 P20988 INFO Epoch 57/70, training loss: 0.02750 [3.25s]\n",
      "2025-04-25 16:02:30,877 P20988 INFO Epoch 58/70, training loss: 0.01285 [3.27s]\n",
      "2025-04-25 16:02:34,161 P20988 INFO Epoch 59/70, training loss: 0.01291 [3.28s]\n",
      "2025-04-25 16:02:39,877 P20988 INFO  testing loss: 0.04131 \n",
      "2025-04-25 16:02:39,879 P20988 INFO Test -- F1: 0.9897, Rec: 0.9922, Pre: 0.9873, HR@1: 0.9907, ndcg@1: 0.9907, HR@3: 0.9915, ndcg@3: 0.9912, HR@5: 0.9922, ndcg@5: 0.9915\n",
      "2025-04-25 16:02:43,190 P20988 INFO Epoch 60/70, training loss: 0.03550 [3.29s]\n",
      "2025-04-25 16:02:46,420 P20988 INFO Epoch 61/70, training loss: 0.07607 [3.23s]\n",
      "2025-04-25 16:02:49,720 P20988 INFO Epoch 62/70, training loss: 0.03625 [3.30s]\n",
      "2025-04-25 16:02:53,004 P20988 INFO Epoch 63/70, training loss: 0.02072 [3.28s]\n",
      "2025-04-25 16:02:56,266 P20988 INFO Epoch 64/70, training loss: 0.01288 [3.26s]\n",
      "2025-04-25 16:02:59,564 P20988 INFO Epoch 65/70, training loss: 0.01038 [3.30s]\n",
      "2025-04-25 16:03:02,765 P20988 INFO Epoch 66/70, training loss: 0.00696 [3.20s]\n",
      "2025-04-25 16:03:05,985 P20988 INFO Epoch 67/70, training loss: 0.00489 [3.22s]\n",
      "2025-04-25 16:03:09,289 P20988 INFO Epoch 68/70, training loss: 0.00300 [3.30s]\n",
      "2025-04-25 16:03:12,544 P20988 INFO Epoch 69/70, training loss: 0.00185 [3.25s]\n",
      "2025-04-25 16:03:18,425 P20988 INFO  testing loss: 0.02673 \n",
      "2025-04-25 16:03:18,427 P20988 INFO Test -- F1: 0.9929, Rec: 0.9957, Pre: 0.9901, HR@1: 0.9943, ndcg@1: 0.9943, HR@3: 0.9950, ndcg@3: 0.9948, HR@5: 0.9957, ndcg@5: 0.9951\n",
      "2025-04-25 16:03:21,778 P20988 INFO Epoch 70/70, training loss: 0.00192 [3.33s]\n",
      "2025-04-25 16:03:21,779 P20988 INFO * Best result got at epoch 49 with HR@1: 0.9957\n",
      "2025-04-25 16:03:21,783 P20988 INFO Current hash_id b52b1a7d\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import dgl\n",
    "class chunkDataset(Dataset): #[node_num, T, else]\n",
    "    \"\"\"\n",
    "           初始化函数，用于构建图数据结构并存储相关数据。\n",
    "\n",
    "           Args:\n",
    "               chunks (dict): 包含多个数据块的字典，每个数据块包含日志、指标、追踪信息以及对应的错误标签。\n",
    "               node_num (int): 图中节点的数量。\n",
    "               edges (tuple): 图的边信息，包含两个列表，分别表示边的源节点和目标节点。\n",
    "           \"\"\"\n",
    "    def __init__(self, chunks, node_num, edges):\n",
    "        # 存储图数据及其对应的错误标签\n",
    "        self.data = []\n",
    "        # 用于将索引映射到数据块ID的字典\n",
    "        self.idx2id = {}\n",
    "        # 遍历chunks字典，构建图数据结构并存储相关信息\n",
    "        for idx, chunk_id in enumerate(chunks.keys()):\n",
    "            # 将索引映射到数据块ID\n",
    "            self.idx2id[idx] = chunk_id\n",
    "            chunk = chunks[chunk_id]\n",
    "            # 使用DGL库创建有向图，并设置节点特征，edges[0] 和 edges[1] 分别表示图中的源节点和目标节点。0->1\n",
    "            graph = dgl.graph((edges[0], edges[1]), num_nodes=node_num)\n",
    "            # 设置节点的日志特征，graph.ndata 是一个字典，用于存储图中节点的特征数据。每个键对应一个特征名称，值是一个张量（tensor），表示所有节点在该特征上的值\n",
    "            # torch.FloatTensor 是 PyTorch 中的一个函数，用于将输入数据转换为浮点型张量（tensor）。\n",
    "            graph.ndata[\"logs\"] = torch.FloatTensor(chunk[\"logs\"])\n",
    "            # 设置节点的指标特征\n",
    "            graph.ndata[\"metrics\"] = torch.FloatTensor(chunk[\"metrics\"])\n",
    "            # 设置节点的追踪特征\n",
    "            graph.ndata[\"traces\"] = torch.FloatTensor(chunk[\"traces\"])\n",
    "            # 将图及其对应的错误节点存储到data列表中\n",
    "            # 如果 chunk[\"culprit\"] 为 -1，表示该数据块中没有故障节点。\n",
    "            # 否则，chunk[\"culprit\"] 表示故障节点的索引（从 0 开始）\n",
    "            # 这样做的目的是将每个数据块的图结构和对应的标签组合在一起，形成一个完整的数据项，方便后续的数据加载和处理。\n",
    "            self.data.append((graph, chunk[\"culprit\"]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    def __get_chunk_id__(self, idx):\n",
    "        return self.idx2id[idx]\n",
    "\n",
    "from utils import *\n",
    "from base import BaseModel\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--random_seed\", default=42, type=int)\n",
    "\n",
    "### Training params\n",
    "parser.add_argument(\"--gpu\", default=True, type=lambda x: x.lower() == \"true\")\n",
    "parser.add_argument(\"--epoches\", default=70, type=int)\n",
    "parser.add_argument(\"--batch_size\", default=256, type=int)\n",
    "parser.add_argument(\"--lr\", default=0.001, type=float)\n",
    "parser.add_argument(\"--patience\", default=10, type=int)\n",
    "parser.add_argument(\"--node_feat_dim\", default=64, type=int)\n",
    "\n",
    "##### Fuse params\n",
    "parser.add_argument(\"--self_attn\", default=True, type=lambda x: x.lower() == \"true\")\n",
    "# parser.add_argument(\"--self_attn\", default=False, type=lambda x: x.lower() == \"tru e\")\n",
    "parser.add_argument(\"--fuse_dim\", default=128, type=int)\n",
    "parser.add_argument(\"--alpha\", default=0.5, type=float)\n",
    "parser.add_argument(\"--beta\", default=0.1, type=float)\n",
    "parser.add_argument(\"--locate_hiddens\", default=[64], type=int, nargs='+')\n",
    "parser.add_argument(\"--detect_hiddens\", default=[64], type=int, nargs='+')\n",
    "parser.add_argument(\"--detector_rank\", default=16, type=int)\n",
    "parser.add_argument(\"--locator_rank\", default=16, type=int)\n",
    "\n",
    "##### Source params\n",
    "parser.add_argument(\"--log_dim\", default=16, type=int)\n",
    "parser.add_argument(\"--trace_kernel_sizes\", default=[2], type=int, nargs='+')\n",
    "parser.add_argument(\"--trace_hiddens\", default=[64], type=int, nargs='+')\n",
    "parser.add_argument(\"--metric_kernel_sizes\", default=[2], type=int, nargs='+')\n",
    "parser.add_argument(\"--metric_hiddens\", default=[64], type=int, nargs='+')\n",
    "parser.add_argument(\"--graph_hiddens\", default=[64], type=int, nargs='+')\n",
    "parser.add_argument(\"--attn_head\", default=4, type=int, help=\"For gat or gat-v2\")\n",
    "parser.add_argument(\"--activation\", default=0.2, type=float, help=\"use LeakyReLU, shoule be in (0,1)\")\n",
    "\n",
    "##### TimesNet-specific params\n",
    "parser.add_argument(\"--seq_len\", default=None, type=int, help=\"Input sequence length for TimesNet\")\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument(\"--pred_len\", default=0, type=int, help=\"Prediction length for TimesNet\")\n",
    "parser.add_argument(\"--enc_in\", default=2, type=int, help=\"Input feature dimension for TimesNet\")\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF', help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--freq', type=str, default='h', help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument(\"--c_out\", default=2, type=int, help=\"Output feature dimension for TimesNet\")\n",
    "parser.add_argument(\"--d_model\", default=2, type=int, help=\"Model hidden dimension for TimesNet\")\n",
    "parser.add_argument(\"--d_ff\", default=256, type=int, help=\"Feed-forward network dimension for TimesNet\")\n",
    "parser.add_argument(\"--num_kernels\", default=3, type=int, help=\"Number of convolutional kernels for TimesNet\")\n",
    "parser.add_argument(\"--e_layers\", default=2, type=int, help=\"Number of TimesNet layers\")\n",
    "parser.add_argument(\"--task_name\", default=\"anomaly_detection\", type=str, help=\"Task type for TimesNet\")\n",
    "parser.add_argument(\"--top_k\", default=3, type=int, help=\"Top-k frequencies to extract in FFT\")\n",
    "\n",
    "\n",
    "##### Data params\n",
    "parser.add_argument(\"--data\", type=str, default=\"SN\")\n",
    "parser.add_argument(\"--result_dir\", default=\"../result/\")\n",
    "\n",
    "### add_module\n",
    "parser.add_argument(\"--use_transformer\", default=True, type=lambda x: x.lower() == \"true\", help=\"Use TransformerEncoder for TraceModel and MetricModel\")\n",
    "parser.add_argument(\"--use_CGLU\", default=True, type=lambda x: x.lower() == \"true\", help=\"Use CGLU for MultsourceEncoder\")\n",
    "parser.add_argument(\"--use_TraceDifussion\", default=True, type=lambda x: x.lower() == \"true\", help=\"Use CGLU for GraphModel\")\n",
    "parser.add_argument(\"--use_crossModalAttenion\", default=False, type=lambda x: x.lower() == \"true\", help=\"Use crossModalAttenion for MultsourceEncoder\")\n",
    "parser.add_argument(\"--use_EvolveGCN\", default=False, type=lambda x: x.lower() == \"true\", help=\"Use use_EvolveGCN for GraphModel\")\n",
    "\n",
    "args, unknown_args = parser.parse_known_args()  # 替换 your_dataset_name\n",
    "params = vars(args)  # 正确操作：将命名空间对象转换为 dict\n",
    "\n",
    "import logging\n",
    "def get_device(gpu):\n",
    "    if gpu and torch.cuda.is_available():\n",
    "        logging.info(\"Using GPU...\")\n",
    "        return torch.device(\"cuda\")\n",
    "    logging.info(\"Using CPU...\")\n",
    "    return torch.device(\"cpu\")\n",
    "    \n",
    "\n",
    "def collate(data):\n",
    "    \"\"\"\n",
    "       对数据进行整理，将图形数据和标签分离并批处理。\n",
    "\n",
    "       参数:\n",
    "       - data: 一个列表，其中包含多个元组，每个元组包含一个图和对应的标签。\n",
    "\n",
    "       返回:\n",
    "       - batched_graph: 批处理后的图数据。\n",
    "       - torch.tensor(labels): 标签的张量表示。\n",
    "       \"\"\"\n",
    "    graphs, labels = map(list, zip(*data))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph , torch.tensor(labels)\n",
    "\n",
    "def run(evaluation_epoch=10):\n",
    "    # 设定数据路径\n",
    "    data_dir = os.path.join(\"./chunks\", params[\"data\"])\n",
    "    # 从json文件中加载元数据\n",
    "    metadata = read_json(os.path.join(data_dir, \"metadata.json\"))\n",
    "    # 从元数据中获取事件数量、节点数量和指标数量、边信息\n",
    "    event_num, node_num, metric_num =  metadata[\"event_num\"], metadata[\"node_num\"], metadata[\"metric_num\"]\n",
    "    edges = metadata[\"edges\"]\n",
    "    # 读取元数据中chunk长度并设置进参数中\n",
    "    params[\"chunk_lenth\"] = metadata[\"chunk_lenth\"]\n",
    "\n",
    "    ############# 如果未指定 seq_len，则使用 chunk_lenth #########\n",
    "    if params[\"seq_len\"] is None:\n",
    "        params[\"seq_len\"] = params[\"chunk_lenth\"]\n",
    "\n",
    "    # 生成实验唯一标识并配置运行环境\n",
    "    # 包含参数哈希化、随机种子固定、GPU设备选择等初始化操作\n",
    "    hash_id = dump_params(params)\n",
    "    params[\"hash_id\"] = hash_id\n",
    "    seed_everything(params[\"random_seed\"])\n",
    "    device = get_device(params[\"gpu\"])\n",
    "\n",
    "    #加载预处理后的数据块\n",
    "    train_chunks, test_chunks = load_chunks(data_dir)\n",
    "    # 构建数据集和数据加载器\n",
    "    # 使用自定义的chunkDataset处理图结构数据\n",
    "    train_data = chunkDataset(train_chunks, node_num, edges)\n",
    "    test_data = chunkDataset(test_chunks, node_num, edges)\n",
    "    # 动态计算总特征维度\n",
    "    graph_example = train_data[0][0]  # 获取第一个图样本\n",
    "    feature_keys = ['logs', 'metrics', 'traces']\n",
    "    total_node_feat_dim = sum([\n",
    "        graph_example.ndata[key].view(graph_example.ndata[key].size(0), -1).size(1)  # 展平后计算维度\n",
    "        for key in feature_keys\n",
    "    ])\n",
    "    params[\"node_feat_dim\"] = total_node_feat_dim  # 设置总特征维度\n",
    "\n",
    "\n",
    "    train_dl = DataLoader(train_data, batch_size=params[\"batch_size\"], shuffle=True, collate_fn=collate, pin_memory=True)\n",
    "    test_dl = DataLoader(test_data, batch_size=params[\"batch_size\"], shuffle=True, collate_fn=collate, pin_memory=True)\n",
    "    # 初始化基础模型并启动训练流程\n",
    "    model = BaseModel(event_num, metric_num, node_num, device, **params)\n",
    "    # fit方法返回评估指标得分和收敛状\n",
    "    scores, converge = model.fit(train_dl, test_dl, evaluation_epoch=evaluation_epoch)\n",
    "    module_info = f\"Transformer: {params['use_transformer']}, \" \\\n",
    "                  f\"TraceDifussion: {params['use_TraceDifussion']}, \" \\\n",
    "                  f\"CGLU: {params['use_CGLU']}, \" \\\n",
    "                  f\"crossModalAttenion: { params['use_crossModalAttenion']}, \" \\\n",
    "                  f\"EvolveGCN: {params['use_EvolveGCN']} \"\n",
    "\n",
    "\n",
    "        # 将实验结果保存到指定目录下，并记录实验唯一标识\n",
    "    dump_scores(params[\"result_dir\"], hash_id, scores, converge, params[\"data\"], params[\"epoches\"], params[\"lr\"], params[\"gpu\"], module_info)\n",
    "    logging.info(\"Current hash_id {}\".format(hash_id))\n",
    "\n",
    "if \"__main__\" == __name__:\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f100348e-5150-4eff-9982-80dbda4b22b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 16:04:37,412 P20988 INFO Using GPU...\n",
      "2025-04-25 16:04:37,413 P20988 INFO Load from ./chunks\\TT\n",
      "2025-04-25 16:05:38,186 P20988 INFO Model Parameters: hash_id=bfb023f3, data=TT, device=cuda, lr=0.001, epoches=70\n",
      "2025-04-25 16:06:26,738 P20988 INFO Epoch 1/70, training loss: 0.74033 [48.55s]\n",
      "2025-04-25 16:07:08,768 P20988 INFO Epoch 2/70, training loss: 0.19566 [42.03s]\n",
      "2025-04-25 16:07:49,918 P20988 INFO Epoch 3/70, training loss: 0.17215 [41.15s]\n",
      "2025-04-25 16:08:33,290 P20988 INFO Epoch 4/70, training loss: 0.18001 [43.37s]\n",
      "2025-04-25 16:09:16,001 P20988 INFO Epoch 5/70, training loss: 0.14360 [42.71s]\n",
      "2025-04-25 16:09:59,271 P20988 INFO Epoch 6/70, training loss: 0.14718 [43.27s]\n",
      "2025-04-25 16:10:42,160 P20988 INFO Epoch 7/70, training loss: 0.14127 [42.89s]\n",
      "2025-04-25 16:11:26,376 P20988 INFO Epoch 8/70, training loss: 0.10565 [44.21s]\n",
      "2025-04-25 16:12:08,523 P20988 INFO Epoch 9/70, training loss: 0.09727 [42.15s]\n",
      "2025-04-25 16:13:13,250 P20988 INFO  testing loss: 0.11101 \n",
      "2025-04-25 16:13:13,252 P20988 INFO Test -- F1: 0.9591, Rec: 0.9736, Pre: 0.9450, HR@1: 0.9676, ndcg@1: 0.9676, HR@3: 0.9722, ndcg@3: 0.9703, HR@5: 0.9729, ndcg@5: 0.9706\n",
      "2025-04-25 16:13:54,820 P20988 INFO Epoch 10/70, training loss: 0.09716 [41.52s]\n",
      "2025-04-25 16:14:37,798 P20988 INFO Epoch 11/70, training loss: 0.07742 [42.97s]\n",
      "2025-04-25 16:15:19,579 P20988 INFO Epoch 12/70, training loss: 0.07092 [41.78s]\n",
      "2025-04-25 16:16:02,836 P20988 INFO Epoch 13/70, training loss: 0.09358 [43.26s]\n",
      "2025-04-25 16:16:46,176 P20988 INFO Epoch 14/70, training loss: 0.07584 [43.34s]\n",
      "2025-04-25 16:17:29,361 P20988 INFO Epoch 15/70, training loss: 0.06907 [43.18s]\n",
      "2025-04-25 16:18:11,001 P20988 INFO Epoch 16/70, training loss: 0.05842 [41.64s]\n",
      "2025-04-25 16:18:52,581 P20988 INFO Epoch 17/70, training loss: 0.07185 [41.58s]\n",
      "2025-04-25 16:19:34,552 P20988 INFO Epoch 18/70, training loss: 0.05862 [41.97s]\n",
      "2025-04-25 16:20:16,632 P20988 INFO Epoch 19/70, training loss: 0.05563 [42.08s]\n",
      "2025-04-25 16:21:21,064 P20988 INFO  testing loss: 0.04571 \n",
      "2025-04-25 16:21:21,065 P20988 INFO Test -- F1: 0.9800, Rec: 0.9804, Pre: 0.9797, HR@1: 0.9798, ndcg@1: 0.9798, HR@3: 0.9804, ndcg@3: 0.9802, HR@5: 0.9804, ndcg@5: 0.9802\n",
      "2025-04-25 16:22:04,650 P20988 INFO Epoch 20/70, training loss: 0.05295 [43.54s]\n",
      "2025-04-25 16:22:50,631 P20988 INFO Epoch 21/70, training loss: 0.07132 [45.98s]\n",
      "2025-04-25 16:23:32,763 P20988 INFO Epoch 22/70, training loss: 0.05899 [42.13s]\n",
      "2025-04-25 16:24:17,357 P20988 INFO Epoch 23/70, training loss: 0.06355 [44.59s]\n",
      "2025-04-25 16:24:59,022 P20988 INFO Epoch 24/70, training loss: 0.04848 [41.66s]\n",
      "2025-04-25 16:25:42,004 P20988 INFO Epoch 25/70, training loss: 0.04965 [42.98s]\n",
      "2025-04-25 16:26:23,778 P20988 INFO Epoch 26/70, training loss: 0.04076 [41.77s]\n",
      "2025-04-25 16:27:05,397 P20988 INFO Epoch 27/70, training loss: 0.03929 [41.62s]\n",
      "2025-04-25 16:27:47,806 P20988 INFO Epoch 28/70, training loss: 0.03745 [42.41s]\n",
      "2025-04-25 16:28:33,689 P20988 INFO Epoch 29/70, training loss: 0.04292 [45.88s]\n",
      "2025-04-25 16:29:36,758 P20988 INFO  testing loss: 0.03741 \n",
      "2025-04-25 16:29:36,759 P20988 INFO Test -- F1: 0.9839, Rec: 0.9849, Pre: 0.9830, HR@1: 0.9847, ndcg@1: 0.9847, HR@3: 0.9849, ndcg@3: 0.9848, HR@5: 0.9849, ndcg@5: 0.9848\n",
      "2025-04-25 16:30:19,908 P20988 INFO Epoch 30/70, training loss: 0.03731 [43.10s]\n",
      "2025-04-25 16:31:02,746 P20988 INFO Epoch 31/70, training loss: 0.05575 [42.84s]\n",
      "2025-04-25 16:31:44,822 P20988 INFO Epoch 32/70, training loss: 0.03803 [42.07s]\n",
      "2025-04-25 16:32:27,443 P20988 INFO Epoch 33/70, training loss: 0.04945 [42.62s]\n",
      "2025-04-25 16:33:40,363 P20988 INFO Epoch 34/70, training loss: 0.04334 [72.92s]\n",
      "2025-04-25 16:34:40,276 P20988 INFO Epoch 35/70, training loss: 0.03639 [59.91s]\n",
      "2025-04-25 16:35:21,974 P20988 INFO Epoch 36/70, training loss: 0.02638 [41.69s]\n",
      "2025-04-25 16:36:03,384 P20988 INFO Epoch 37/70, training loss: 0.02709 [41.41s]\n",
      "2025-04-25 16:36:45,214 P20988 INFO Epoch 38/70, training loss: 0.02932 [41.83s]\n",
      "2025-04-25 16:37:26,560 P20988 INFO Epoch 39/70, training loss: 0.02574 [41.34s]\n",
      "2025-04-25 16:38:29,710 P20988 INFO  testing loss: 0.04837 \n",
      "2025-04-25 16:38:29,711 P20988 INFO Test -- F1: 0.9809, Rec: 0.9771, Pre: 0.9848, HR@1: 0.9759, ndcg@1: 0.9759, HR@3: 0.9771, ndcg@3: 0.9766, HR@5: 0.9771, ndcg@5: 0.9766\n",
      "2025-04-25 16:39:11,683 P20988 INFO Epoch 40/70, training loss: 0.03761 [41.95s]\n",
      "2025-04-25 16:39:53,813 P20988 INFO Epoch 41/70, training loss: 0.03543 [42.13s]\n",
      "2025-04-25 16:40:35,621 P20988 INFO Epoch 42/70, training loss: 0.02969 [41.81s]\n",
      "2025-04-25 16:41:17,173 P20988 INFO Epoch 43/70, training loss: 0.02786 [41.55s]\n",
      "2025-04-25 16:42:00,463 P20988 INFO Epoch 44/70, training loss: 0.03175 [43.29s]\n",
      "2025-04-25 16:42:42,035 P20988 INFO Epoch 45/70, training loss: 0.02876 [41.57s]\n",
      "2025-04-25 16:43:23,810 P20988 INFO Epoch 46/70, training loss: 0.02071 [41.77s]\n",
      "2025-04-25 16:44:05,424 P20988 INFO Epoch 47/70, training loss: 0.02035 [41.61s]\n",
      "2025-04-25 16:44:47,158 P20988 INFO Epoch 48/70, training loss: 0.01794 [41.73s]\n",
      "2025-04-25 16:45:28,710 P20988 INFO Epoch 49/70, training loss: 0.03326 [41.55s]\n",
      "2025-04-25 16:46:31,824 P20988 INFO  testing loss: 0.02810 \n",
      "2025-04-25 16:46:31,825 P20988 INFO Test -- F1: 0.9895, Rec: 0.9890, Pre: 0.9900, HR@1: 0.9870, ndcg@1: 0.9870, HR@3: 0.9890, ndcg@3: 0.9883, HR@5: 0.9890, ndcg@5: 0.9883\n",
      "2025-04-25 16:47:14,198 P20988 INFO Epoch 50/70, training loss: 0.01705 [42.33s]\n",
      "2025-04-25 16:47:56,151 P20988 INFO Epoch 51/70, training loss: 0.02434 [41.95s]\n",
      "2025-04-25 16:48:38,268 P20988 INFO Epoch 52/70, training loss: 0.03517 [42.11s]\n",
      "2025-04-25 16:49:19,971 P20988 INFO Epoch 53/70, training loss: 0.02516 [41.70s]\n",
      "2025-04-25 16:50:01,675 P20988 INFO Epoch 54/70, training loss: 0.01583 [41.70s]\n",
      "2025-04-25 16:50:44,026 P20988 INFO Epoch 55/70, training loss: 0.01634 [42.35s]\n",
      "2025-04-25 16:51:26,590 P20988 INFO Epoch 56/70, training loss: 0.01512 [42.56s]\n",
      "2025-04-25 16:52:09,126 P20988 INFO Epoch 57/70, training loss: 0.02054 [42.53s]\n",
      "2025-04-25 16:52:51,105 P20988 INFO Epoch 58/70, training loss: 0.01720 [41.98s]\n",
      "2025-04-25 16:53:32,701 P20988 INFO Epoch 59/70, training loss: 0.01306 [41.59s]\n",
      "2025-04-25 16:54:35,608 P20988 INFO  testing loss: 0.02109 \n",
      "2025-04-25 16:54:35,609 P20988 INFO Test -- F1: 0.9928, Rec: 0.9929, Pre: 0.9927, HR@1: 0.9925, ndcg@1: 0.9925, HR@3: 0.9929, ndcg@3: 0.9927, HR@5: 0.9929, ndcg@5: 0.9927\n",
      "2025-04-25 16:55:17,835 P20988 INFO Epoch 60/70, training loss: 0.03901 [42.18s]\n",
      "2025-04-25 16:55:59,695 P20988 INFO Epoch 61/70, training loss: 0.02322 [41.86s]\n",
      "2025-04-25 16:56:41,651 P20988 INFO Epoch 62/70, training loss: 0.01422 [41.95s]\n",
      "2025-04-25 16:57:23,862 P20988 INFO Epoch 63/70, training loss: 0.01280 [42.21s]\n",
      "2025-04-25 16:58:06,327 P20988 INFO Epoch 64/70, training loss: 0.00888 [42.46s]\n",
      "2025-04-25 16:58:47,981 P20988 INFO Epoch 65/70, training loss: 0.03385 [41.65s]\n",
      "2025-04-25 16:59:29,861 P20988 INFO Epoch 66/70, training loss: 0.01350 [41.88s]\n",
      "2025-04-25 17:00:11,403 P20988 INFO Epoch 67/70, training loss: 0.03916 [41.54s]\n",
      "2025-04-25 17:00:53,268 P20988 INFO Epoch 68/70, training loss: 0.02358 [41.86s]\n",
      "2025-04-25 17:01:34,842 P20988 INFO Epoch 69/70, training loss: 0.01557 [41.57s]\n",
      "2025-04-25 17:02:38,143 P20988 INFO  testing loss: 0.02236 \n",
      "2025-04-25 17:02:38,144 P20988 INFO Test -- F1: 0.9915, Rec: 0.9921, Pre: 0.9909, HR@1: 0.9918, ndcg@1: 0.9918, HR@3: 0.9921, ndcg@3: 0.9920, HR@5: 0.9921, ndcg@5: 0.9920\n",
      "2025-04-25 17:03:20,204 P20988 INFO Epoch 70/70, training loss: 0.01521 [42.04s]\n",
      "2025-04-25 17:03:20,206 P20988 INFO * Best result got at epoch 59 with HR@1: 0.9925\n",
      "2025-04-25 17:03:20,210 P20988 INFO Current hash_id bfb023f3\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import dgl\n",
    "class chunkDataset(Dataset): #[node_num, T, else]\n",
    "    \"\"\"\n",
    "           初始化函数，用于构建图数据结构并存储相关数据。\n",
    "\n",
    "           Args:\n",
    "               chunks (dict): 包含多个数据块的字典，每个数据块包含日志、指标、追踪信息以及对应的错误标签。\n",
    "               node_num (int): 图中节点的数量。\n",
    "               edges (tuple): 图的边信息，包含两个列表，分别表示边的源节点和目标节点。\n",
    "           \"\"\"\n",
    "    def __init__(self, chunks, node_num, edges):\n",
    "        # 存储图数据及其对应的错误标签\n",
    "        self.data = []\n",
    "        # 用于将索引映射到数据块ID的字典\n",
    "        self.idx2id = {}\n",
    "        # 遍历chunks字典，构建图数据结构并存储相关信息\n",
    "        for idx, chunk_id in enumerate(chunks.keys()):\n",
    "            # 将索引映射到数据块ID\n",
    "            self.idx2id[idx] = chunk_id\n",
    "            chunk = chunks[chunk_id]\n",
    "            # 使用DGL库创建有向图，并设置节点特征，edges[0] 和 edges[1] 分别表示图中的源节点和目标节点。0->1\n",
    "            graph = dgl.graph((edges[0], edges[1]), num_nodes=node_num)\n",
    "            # 设置节点的日志特征，graph.ndata 是一个字典，用于存储图中节点的特征数据。每个键对应一个特征名称，值是一个张量（tensor），表示所有节点在该特征上的值\n",
    "            # torch.FloatTensor 是 PyTorch 中的一个函数，用于将输入数据转换为浮点型张量（tensor）。\n",
    "            graph.ndata[\"logs\"] = torch.FloatTensor(chunk[\"logs\"])\n",
    "            # 设置节点的指标特征\n",
    "            graph.ndata[\"metrics\"] = torch.FloatTensor(chunk[\"metrics\"])\n",
    "            # 设置节点的追踪特征\n",
    "            graph.ndata[\"traces\"] = torch.FloatTensor(chunk[\"traces\"])\n",
    "            # 将图及其对应的错误节点存储到data列表中\n",
    "            # 如果 chunk[\"culprit\"] 为 -1，表示该数据块中没有故障节点。\n",
    "            # 否则，chunk[\"culprit\"] 表示故障节点的索引（从 0 开始）\n",
    "            # 这样做的目的是将每个数据块的图结构和对应的标签组合在一起，形成一个完整的数据项，方便后续的数据加载和处理。\n",
    "            self.data.append((graph, chunk[\"culprit\"]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    def __get_chunk_id__(self, idx):\n",
    "        return self.idx2id[idx]\n",
    "\n",
    "from utils import *\n",
    "from base import BaseModel\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--random_seed\", default=42, type=int)\n",
    "\n",
    "### Training params\n",
    "parser.add_argument(\"--gpu\", default=True, type=lambda x: x.lower() == \"true\")\n",
    "parser.add_argument(\"--epoches\", default=70, type=int)\n",
    "parser.add_argument(\"--batch_size\", default=256, type=int)\n",
    "parser.add_argument(\"--lr\", default=0.001, type=float)\n",
    "parser.add_argument(\"--patience\", default=10, type=int)\n",
    "parser.add_argument(\"--node_feat_dim\", default=64, type=int)\n",
    "\n",
    "##### Fuse params\n",
    "parser.add_argument(\"--self_attn\", default=True, type=lambda x: x.lower() == \"true\")\n",
    "# parser.add_argument(\"--self_attn\", default=False, type=lambda x: x.lower() == \"tru e\")\n",
    "parser.add_argument(\"--fuse_dim\", default=128, type=int)\n",
    "parser.add_argument(\"--alpha\", default=0.5, type=float)\n",
    "parser.add_argument(\"--beta\", default=0.1, type=float)\n",
    "parser.add_argument(\"--locate_hiddens\", default=[64], type=int, nargs='+')\n",
    "parser.add_argument(\"--detect_hiddens\", default=[64], type=int, nargs='+')\n",
    "parser.add_argument(\"--detector_rank\", default=16, type=int)\n",
    "parser.add_argument(\"--locator_rank\", default=16, type=int)\n",
    "\n",
    "##### Source params\n",
    "parser.add_argument(\"--log_dim\", default=16, type=int)\n",
    "parser.add_argument(\"--trace_kernel_sizes\", default=[2], type=int, nargs='+')\n",
    "parser.add_argument(\"--trace_hiddens\", default=[64], type=int, nargs='+')\n",
    "parser.add_argument(\"--metric_kernel_sizes\", default=[2], type=int, nargs='+')\n",
    "parser.add_argument(\"--metric_hiddens\", default=[64], type=int, nargs='+')\n",
    "parser.add_argument(\"--graph_hiddens\", default=[64], type=int, nargs='+')\n",
    "parser.add_argument(\"--attn_head\", default=4, type=int, help=\"For gat or gat-v2\")\n",
    "parser.add_argument(\"--activation\", default=0.2, type=float, help=\"use LeakyReLU, shoule be in (0,1)\")\n",
    "\n",
    "##### TimesNet-specific params\n",
    "parser.add_argument(\"--seq_len\", default=None, type=int, help=\"Input sequence length for TimesNet\")\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument(\"--pred_len\", default=0, type=int, help=\"Prediction length for TimesNet\")\n",
    "parser.add_argument(\"--enc_in\", default=2, type=int, help=\"Input feature dimension for TimesNet\")\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF', help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--freq', type=str, default='h', help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument(\"--c_out\", default=2, type=int, help=\"Output feature dimension for TimesNet\")\n",
    "parser.add_argument(\"--d_model\", default=2, type=int, help=\"Model hidden dimension for TimesNet\")\n",
    "parser.add_argument(\"--d_ff\", default=256, type=int, help=\"Feed-forward network dimension for TimesNet\")\n",
    "parser.add_argument(\"--num_kernels\", default=3, type=int, help=\"Number of convolutional kernels for TimesNet\")\n",
    "parser.add_argument(\"--e_layers\", default=2, type=int, help=\"Number of TimesNet layers\")\n",
    "parser.add_argument(\"--task_name\", default=\"anomaly_detection\", type=str, help=\"Task type for TimesNet\")\n",
    "parser.add_argument(\"--top_k\", default=3, type=int, help=\"Top-k frequencies to extract in FFT\")\n",
    "\n",
    "\n",
    "##### Data params\n",
    "parser.add_argument(\"--data\", type=str, default=\"TT\")\n",
    "parser.add_argument(\"--result_dir\", default=\"../result/\")\n",
    "\n",
    "### add_module\n",
    "parser.add_argument(\"--use_transformer\", default=True, type=lambda x: x.lower() == \"true\", help=\"Use TransformerEncoder for TraceModel and MetricModel\")\n",
    "parser.add_argument(\"--use_CGLU\", default=True, type=lambda x: x.lower() == \"true\", help=\"Use CGLU for MultsourceEncoder\")\n",
    "parser.add_argument(\"--use_TraceDifussion\", default=True, type=lambda x: x.lower() == \"true\", help=\"Use CGLU for GraphModel\")\n",
    "parser.add_argument(\"--use_crossModalAttenion\", default=False, type=lambda x: x.lower() == \"true\", help=\"Use crossModalAttenion for MultsourceEncoder\")\n",
    "parser.add_argument(\"--use_EvolveGCN\", default=False, type=lambda x: x.lower() == \"true\", help=\"Use use_EvolveGCN for GraphModel\")\n",
    "\n",
    "args, unknown_args = parser.parse_known_args()  # 替换 your_dataset_name\n",
    "params = vars(args)  # 正确操作：将命名空间对象转换为 dict\n",
    "\n",
    "import logging\n",
    "def get_device(gpu):\n",
    "    if gpu and torch.cuda.is_available():\n",
    "        logging.info(\"Using GPU...\")\n",
    "        return torch.device(\"cuda\")\n",
    "    logging.info(\"Using CPU...\")\n",
    "    return torch.device(\"cpu\")\n",
    "    \n",
    "\n",
    "def collate(data):\n",
    "    \"\"\"\n",
    "       对数据进行整理，将图形数据和标签分离并批处理。\n",
    "\n",
    "       参数:\n",
    "       - data: 一个列表，其中包含多个元组，每个元组包含一个图和对应的标签。\n",
    "\n",
    "       返回:\n",
    "       - batched_graph: 批处理后的图数据。\n",
    "       - torch.tensor(labels): 标签的张量表示。\n",
    "       \"\"\"\n",
    "    graphs, labels = map(list, zip(*data))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph , torch.tensor(labels)\n",
    "\n",
    "def run(evaluation_epoch=10):\n",
    "    # 设定数据路径\n",
    "    data_dir = os.path.join(\"./chunks\", params[\"data\"])\n",
    "    # 从json文件中加载元数据\n",
    "    metadata = read_json(os.path.join(data_dir, \"metadata.json\"))\n",
    "    # 从元数据中获取事件数量、节点数量和指标数量、边信息\n",
    "    event_num, node_num, metric_num =  metadata[\"event_num\"], metadata[\"node_num\"], metadata[\"metric_num\"]\n",
    "    edges = metadata[\"edges\"]\n",
    "    # 读取元数据中chunk长度并设置进参数中\n",
    "    params[\"chunk_lenth\"] = metadata[\"chunk_lenth\"]\n",
    "\n",
    "    ############# 如果未指定 seq_len，则使用 chunk_lenth #########\n",
    "    if params[\"seq_len\"] is None:\n",
    "        params[\"seq_len\"] = params[\"chunk_lenth\"]\n",
    "\n",
    "    # 生成实验唯一标识并配置运行环境\n",
    "    # 包含参数哈希化、随机种子固定、GPU设备选择等初始化操作\n",
    "    hash_id = dump_params(params)\n",
    "    params[\"hash_id\"] = hash_id\n",
    "    seed_everything(params[\"random_seed\"])\n",
    "    device = get_device(params[\"gpu\"])\n",
    "\n",
    "    #加载预处理后的数据块\n",
    "    train_chunks, test_chunks = load_chunks(data_dir)\n",
    "    # 构建数据集和数据加载器\n",
    "    # 使用自定义的chunkDataset处理图结构数据\n",
    "    train_data = chunkDataset(train_chunks, node_num, edges)\n",
    "    test_data = chunkDataset(test_chunks, node_num, edges)\n",
    "    # 动态计算总特征维度\n",
    "    graph_example = train_data[0][0]  # 获取第一个图样本\n",
    "    feature_keys = ['logs', 'metrics', 'traces']\n",
    "    total_node_feat_dim = sum([\n",
    "        graph_example.ndata[key].view(graph_example.ndata[key].size(0), -1).size(1)  # 展平后计算维度\n",
    "        for key in feature_keys\n",
    "    ])\n",
    "    params[\"node_feat_dim\"] = total_node_feat_dim  # 设置总特征维度\n",
    "\n",
    "\n",
    "    train_dl = DataLoader(train_data, batch_size=params[\"batch_size\"], shuffle=True, collate_fn=collate, pin_memory=True)\n",
    "    test_dl = DataLoader(test_data, batch_size=params[\"batch_size\"], shuffle=True, collate_fn=collate, pin_memory=True)\n",
    "    # 初始化基础模型并启动训练流程\n",
    "    model = BaseModel(event_num, metric_num, node_num, device, **params)\n",
    "    # fit方法返回评估指标得分和收敛状\n",
    "    scores, converge = model.fit(train_dl, test_dl, evaluation_epoch=evaluation_epoch)\n",
    "    module_info = f\"Transformer: {params['use_transformer']}, \" \\\n",
    "                  f\"TraceDifussion: {params['use_TraceDifussion']}, \" \\\n",
    "                  f\"CGLU: {params['use_CGLU']}, \" \\\n",
    "                  f\"crossModalAttenion: { params['use_crossModalAttenion']}, \" \\\n",
    "                  f\"EvolveGCN: {params['use_EvolveGCN']} \"\n",
    "\n",
    "\n",
    "        # 将实验结果保存到指定目录下，并记录实验唯一标识\n",
    "    dump_scores(params[\"result_dir\"], hash_id, scores, converge, params[\"data\"], params[\"epoches\"], params[\"lr\"], params[\"gpu\"], module_info)\n",
    "    logging.info(\"Current hash_id {}\".format(hash_id))\n",
    "\n",
    "if \"__main__\" == __name__:\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d20568-5563-4f5a-85ac-0afd467d5d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:peFAD]",
   "language": "python",
   "name": "conda-env-peFAD-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
